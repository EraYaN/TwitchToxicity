%!TEX program = xelatex
%!TEX spellcheck = en_GB
\documentclass[final]{report}
\input{../../.library/preamble.tex}
\input{../../.library/style.tex}
\addbibresource{../../.library/bibliography.bib}

\begin{document}
\chapter{Uncompleted Results}
\label{ch:Uncompleted Results}

This chapter contains work that is uncompleted that was intended to be part of the project, but for several possible reasons was not included or not finished. That could be not enough time, unsuited tools or we couldn't get it to work. So if one wants to follow up on the project, these are several topics which could be further developed:

\begin{enumerate}
	\item Speech recognition
	\item Training
	\item Classifier
	\item Game metadata
	\item Twitch API fix
\end{enumerate}

\section{SpeechRecognition}
Originally we wanted to analyse the interaction between the streamer and the users, so we needed data from the streamer.
The downloaded audio was used with the python library SpeechRecognition. %https://pypi.python.org/pypi/SpeechRecognition/%.
Different speech recognition tool-kits were looked into:
\begin{enumerate}
\item Python speech recognition 3.7.1 which includes CMU Sphinx as speech recognition software. It is very slow as the processing must happen in real-time.
\item Dragon Speech Recognition Software, which is a professional software package. However it has high price which makes it not viable for our project.
\item Google Cloud Speech API, Google has a free tool that can be used for speech recognition. It is however very slow like all other tools and does not return usable text.
\end{enumerate}

All the tools used work in real-time, so this means they are all very slow (each second of audio takes 1 second to process). Likewise none of them returned usable data that doesn't contain a significant amount of errors.
This is expected as speech recognition is still not at the required level to properly gather reliable data, moreover music is usually played on stream and together with game sounds this interferes significantly with the speech recognition.
We conclude that speech recognition on a massive dataset is still not a viable option.
In the near future when services are cheaper and the recognition technology is more advanced it could become interesting to look into this.

\section{Training}
To analyse the chat data, we had to gather a training set in which the chat is rated based on their toxicity.
We converted the compressed pickle file to a chat log from the MLG channel to an SQL insert statement to store the chat into a MySQL database.
A website implemented in JavaScript and PHP showed this chat and we manually rated each chat message with a 1 to 5 (1 being non-toxic, 5 being very toxic). The final training set is also filtered on words that are shorter than 3 letters to remove even more noise. Below the web page where the data can be trained is shown in \ref{fig:rating}.

\begin{figure}[h]
\includegraphics[scale=0.5]{rating}
\caption{Web page to train our dataset}
\label{fig:rating}
\end{figure}

On the far left the message number and the timestamp of the message is shown, together with how often the message has been rated before (between the brackets). In the middle the user (bold) and the message sent by the user can be seen to determine the rating. On the far right are the rating options, with 1 being non toxic and 5 very toxic. To try the rating system yourself, visit https://labs.erayan.com/twitchtoxicity/, using account "bart" with password "wowsuchtwitch".

\clearpage

\section{Classifier}
%NLTK

%From results, about the classifier
After the training set is complete and present in the database it can be used to classify other data. The classifier downloads a sub set of the manually classified data, then extracts word based features and trains a classifier.
The problem was in the initial distribution of all the classes for the classifier.
There are way too many messages rated 1 (non-toxic). Thus the output of the classified was mostly 1.000 (float). Other classifiers that could be used instead of the current Python NLTK are:

\begin{enumerate}
	\item \textbf{VADER Sentiment Analysis}, Valence Aware Dictionary and sEntiment Reasoner. This is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains \cite{vader}.
	\item \textbf{Naive Bayes}, a text classifier based on Naive Bayes. Which are methods of supervised learning algorithms based on applying Bayes' theorem with the "naive" assumption of independence between every pair of features \cite{naivebayes}.
	\item \textbf{MaxEnt}, Maximum Entropy Classifiers. This is a classifier model that is based on the maximum entropy modelling framework. This framework considers all of the probability distributions that are empirically consistent with the training data; and chooses the distribution with the highest entropy \cite{maxent}.
\end{enumerate}

The idea was that the results would give an average degree of toxicity for certain streamers to compare, but also users individually per channel viewed. Based on those results different viewers could also be compared in different channels to see how the toxicity in a channel affects the viewers. So in other words if the toxicity in twitch streams is also contagious.

\section{Game Metadata}

If somehow the game metadata of what is happening in game with timestamps could be obtained it would be highly valuable to give better insight in why the chat responds in certain ways. Speech recognition was one of the options to achieve this, but will always give results with high noise in the data. If this metadata could be obtained from the game itself it would contain no noise. However most company's don't allow this due privacy reasons, so open data from multiplayer games will probably never happen.

\section{Twitch API}

Unfortunately the current Twitch API does not return all videos. For example when trying to download all the videos of the company behind League of Legends, Riotgames, from Twitch, it returns no videos. While there are videos on the riotgames page on the twitch.tv profile website. As this is the responsibility of Twitch there is currently little that can be done about this, in the future when Twitch could have updated their API the video download script might work more optimal.


\end{document}