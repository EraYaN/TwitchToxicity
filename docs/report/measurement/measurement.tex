%!TEX program = xelatex
%!TEX spellcheck = en_GB
\documentclass[final]{report}
\input{../../.library/preamble.tex}
\input{../../.library/style.tex}
\addbibresource{../../.library/bibliography.bib}
\begin{document}
\chapter{Measurement}
\label{ch:measurement}


To acquire the data necessary for this project twitch replay videos where downloaded from twitch using YouTube-DL.
A script was built to do this automatically for the most popular streamers for various games. 
It can download multiple replays in parallel which speeds up the process because twitch throttles the downloads.
Only the chat replay is kept as audio and video analysis is out of scope of this project.

YouTube-DL saves the files in json format,  The chat replays are converted to pickle files to use with python. 
This results in about 1.5GB of pickle files, so the data was compressed to save about 750MB of space.
The first step is to filter all the stored chats with a preprocessor to filter the twitch emotes and remove unusual characters to prepare the data for training and classification.

When analyzing the data the classifier first needs to be trained to properly cast a verdict if a certain sentence is toxic or not.
This was done manually by creating a web page that shows a message from a user and allowing us to decide if it is toxic. This verdict is stored in a large SQL database to use later for classification.


\section{Video Downloader}
First of all, all the video's that are uploaded by the 100 most popular streamers are collected. This is stored in a file which holds the streamer, together with the id's of each video.
This is done using the TwitchVideoIDCollector script. The most popular streamers are hardcoded, but can be extended easily.

After all the video id's are collected, the script TwitchVideoDownloader uses this list to fetch all the video's.
Some filtering is already at place here. The rechat functionality was implemented by Twitch at 2016-02-23, so all that are published before this date is useless since we are mianly interested in the chat.
The downloader also downloaded the video which only contains audio. Later we decided to leave the video/audio, since speechrecognition is still too hard.

The TwitchVideoDownloader return the following files:
\begin{itemize}
\item A thumbnail
\item A json file with metadata of the video 
\item A json file with apiinfo
\item A json file with the chat
\item A mp4 with the video, which only contains audio.
\end{itemize}

\section{Preprocessor}
Before we could train our classifier we must filter the chat. The chat is filled with emotes and ascii art which is not supported by our trainer/classifier.
At first we started off with a python script which removed each word in TwitchEmotes.txt. All detected unicode characters are removed in the same matter.
The TwitchEmotes.txt file was relatively small in the beginning, so the python script could handle all the chat files fast enough. However, later we detected there are a lot more emotes, but also custom emotes by the streamers.
This expanded our text file massively to around 150k words. At this point, the python script needed 4 weeks to filter all chat files with the new emotes list.

So we switched to $C\#$ to implement our preprocessor.
This script was able to filter the chat data within 5 minutes, a major improved compared to 4 weeks.

\section{SpeechRecognition}
Originally we wanted to analyse the interaction between the streamer and the users, so we needed data from the streamer.
The audio downloaded was used with the python library SpeechRecognition %https://pypi.python.org/pypi/SpeechRecognition/%.
But the time needed to analyse an audio file was around 1:1, so 1 minute of audio took 1 minute to parse. The text that was returned was completely gibberish and useless.
We conclude that speechrecognintion on a massive dataset is still not an option. Maybe in 10 years when services are cheap and the recognition is way waster this might be an option.

\section{Training}
%Javascripts

\section{Classifier}
%NLTK


\end{document}